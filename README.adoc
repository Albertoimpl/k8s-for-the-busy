== Kubernetes for the busy developer

The Kubernetes hype train has reached full velocity and many organisations are now adopting the technology.
The Kubernetes ecosystem can be very operator focused and it can be a challenge for developers to distill the information that is relevant for their job.
There are a large number of available tools aimed at solving a broad set of problems but it can be daunting choosing which to invest in.
This tutorial aims to provide developers with a thorough grounding on Kubernetes concepts, suggest best practices and get hands-on with some of the essential tooling.

=== Why this tutorial

After working with a PaaS where the developer has just one single command to deploy the applications: `cf push`, `heroku deploy` or `gcloud app deploy`.
They abstract from a huge amount of complexity.
The experience coming to kubernetes is far from that.
There are many tools for each phase of the lifecycle increasing the learning curve.

=== What this tutorial is not about

* How to operate or deploy a k8s cluster.
* K8s architecture will be treated as a black box and only the necessary will be described.

=== <<containerizing-java/README.adoc#,Building an image from source>>

=== Tagging

We have been tagging our image using the default tag `latest`.
But the same way you would never go to production with `SNAPSHOT` in your dependencies, you will never go with `latest` as your tag.

There are different strategies, the one I like the most is the timestamp one:

```
jib.to.image = 'grc.io/albertoimpl/myapp:' + System.nanoTime()
```

Or on CI you can use the git hash:

```
./gradlew jib --image=grc.io/albertoimpl/myapp:{{github.sha}}
```

=== Registries

In order for our image to be downloaded, we have to upload it somewhere.
Registries are the place where we will upload our images.

The ones we took a look at are:

==== DockerHub

Is the original and most used registry, free for public, paid for private.
They have an on prem offering.

==== GRC/ACR/ECR

Are the biggest cloud providers registries, if you are using their hosted kubernetes container services, you should use their registries.

==== Harbor

Open Source registry started by VMWare, now part of the CNCF. It enables users to have their on prem registry.

=== Deployment

We can get jib to push the image to the registry:

```
 ./gradlew jib --image=albertoimpl/myapp-jib
To honour the JVM settings for this build a new JVM will be forked. Please consider using the daemon: https://docs.gradle.org/5.6.2/userguide/gradle_daemon.html.
Daemon will be stopped at the end of the build stopping after processing

> Task :jib

Containerizing application to albertoimpl/myapp-jib...
The credential helper (docker-credential-desktop) has nothing for server URL: registry-1.docker.io

Got output:

credentials not found in native keychain

The credential helper (docker-credential-desktop) has nothing for server URL: registry.hub.docker.com

Got output:

credentials not found in native keychain


Container entrypoint set to [java, -cp, /app/resources:/app/classes:/app/libs/*, com.albertoimpl.devoxxbe.containers.ContainersApplication]

Built and pushed image as albertoimpl/myapp-jib
Executing tasks:
[==============================] 100.0% complete


BUILD SUCCESSFUL in 34s
3 actionable tasks: 3 executed
```

or by adding the registry to our `build.gradle` file:

```
jib {
	to {
		image = 'albertoimpl/myapp-jib'
	}
}
```

Now that we have a tagged image in a registry, we are ready to deploy it into kubernetes.

=== <<k8s-basics/README.adoc#,K8s basics>>

=== <<local-development-workflow/README.adoc#,Local Development Workflow>>

=== <<observability/README.adoc#,Observability>>

=== Networking

==== Service Discovery

We have now a HA, scalable, easy to debug, monitor and deploy hello world application but reality is that this is far still from a real application.
Usually applications will talk to other external application or services.

We could add databases and services into the same Pod so that we don't have to talk to anything on the outside, but that is a terrible idea since we want each of them to have a different lifecycle.

On previous examples we queried all the services but didn't pay attention to all of them.
Concretely we are going to focus on `kube-dns`

```
k get services -n kube-system
NAME                                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                  AGE
kube-dns                                             ClusterIP   10.96.0.10       <none>        53/UDP,53/TCP,9153/TCP   166m
```

Every Pod we have, if we SSH in we can see how we have a nameserver with the `kube-dns` cluster IP.

```
$ cat /etc/resolv.conf
search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10
options ndots:5
```

That means that in kubernetes, we can just reference our services by name and we will have access to them.

Let's see that in action by creating two apps, one that will provide customer names and one consumer that will welcome them.

```
	@RequestMapping("/customers")
	public List<String> home() {
		return Arrays.asList("Laura", "Bella", "Olga");
	}
```

The deployment will be the same as before but the service is important now:

```
apiVersion: v1
kind: Service
metadata:
  name: service-myapp-provider
  labels:
    app: myapp-provider
spec:
  selector:
    app: myapp-provider
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
  type: NodePort
```

Since the service name we are choosing `service-myapp-provider` is what will be used by our consumer application.

```
curl localhost:8081/customers
["Laura","Bella","Olga"]
```

```
	@Value("${provider.url}")
	private String providerUrl;

	@RequestMapping("/hello")
	public String home() {
		List<String> customers = new RestTemplate().getForObject(providerUrl, List.class);
		String message = customers.stream().collect(Collectors.joining(","));
		return "Welcome: " + message;
	}
```

Having the `provider.url` as:

```
provider.url=http://service-myapp-provider:8080/customers
```

We can see how just specifying the service name `service-myapp-provider` we can access to the provider service:

```
curl localhost:8080/hello
Welcome: Laura,Bella,Olga
```

If the service was in a different namespace we could just reference it by using its Fully Qualified Domain Name: `other-service.other-namespace.svc.cluster.local`

==== Ingress

So far, we have been locally port-forwarding everything or accessing to our services through the IP assigned to us from it's `NodePort`

```
 k get svc
NAME                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
kubernetes               ClusterIP   10.96.0.1      <none>        443/TCP          3d1h
service-myapp-provider   NodePort    10.107.90.53   <none>        8080:31230/TCP   2d21h
```

However, this is not a good approach in order to be consumed from the outside world.
In order to make this nicer for the external world we are going to create an Ingress.
Ingress exposes routes from outside the cluster to services within the cluster.

Let's make our provider app available to the world:

```ingress.yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: provider-ingress
  annotations:
    ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: provider.test.app.com
    - http:
        paths:
          - path: /
            backend:
              serviceName: service-myapp-provider
              servicePort: 8080
```

Ingress supports path and host based routing with a standard entry-point to multiple services.

```
k apply -f networking/ingress.yaml
ingress.networking.k8s.io/provider-ingress created
```

```
k get svc,ingress
NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/kubernetes               ClusterIP   10.96.0.1      <none>        443/TCP          3d1h
service/service-myapp-provider   NodePort    10.107.90.53   <none>        8080:31230/TCP   2d21h

NAME                                  HOSTS                 ADDRESS          PORTS   AGE
ingress.extensions/provider-ingress   provider.test.app.com 107.178.254.228  80      75s
```

```
curl 107.178.254.228/customers
["Laura","Bella","Olga"]
```

Now we have a much better way to process the ingress traffic to our cluster.
Note that to test it we need a real development cluster since KIND does not enable support for this at the moment.

=== Persistent Volumes

Some application and services will require persistent data.
Pod are ephemeral, they will likely die and the data will be removed.
For that, K8s has an amazing feature called persistent volume, which is kind of a folder that won't get lost on a restart.

Typically applications should not need to persist anything on disk not to violate 12 factor app principles.
We are going to use mongo as an example where all the data is kept in the directory `/data/db`

```
volumeMounts:
    - name: mongo-persistent-volume
    - mountPath: /data/db
```

We are going to specify where do we want the data to be and to create the directory if it does not exist:

```
  volumes:
  - name: mongo-persistent-volume
    hostPath:
        path: /mount/mongodbdata
        type: DirectoryOrCreate
```

On this example we used `hostPath` which mounts a file or a directory from the host nodeâ€™s filesystem into your Pod.

```
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: k8s.gcr.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    # This GCE PD must already exist.
    gcePersistentDisk:
      pdName: my-data-disk
      fsType: ext4
```

Sadly, depending on the provider we are using the volume type will vary, and for that reason, Kubernetes added a new abstraction called Persistent Volume Claim.
So, instead of manually creating the volume with our IaaS specific configuration, we should be using Persistent Volume Claims that way we will decouple the creation and administration of those resources.

However, there is another way to create them and it is by having Dynamic Volumes.
We can have a default one so that we don't have to have to create them beforehand.
Careful since you can start requesting a lot of disk if you are not cleaning them and it will increase your IaaS cost.

=== Externalised Configuration

On the previous examples we were just hardcoding our properties in an application.properties file.
A real world application contain different type of configuration: hard, that will never change and are required to be bundled with our app and soft, that may change or that can be changed on runtime, such as A/B tests.
We will see how to pass configuration and secrets to our application.

==== Config Maps

Config Maps are volumes we mount into our containers.

We can create them using the CLI from literals:

```
k create configmap name --from-literal key=value
```

from config files:

```
k create configmap name --from-file=myconfig.properties
```

or as usual with YAML:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: database-configuration
  namespace: default
data:
  MYSQL_DATABASE: "customers"
  MYSQL_HOSTNAME: "database.service"
  MYSQL_PASSWORD: "secretpassword"
  MYSQL_PORT: "3701"
  MYSQL_USER: "customers"
```

If we apply our new configuration:

```
k apply -f configurations/database-configmap.yaml
configmap/database-configuration created
```

We can add our configuration to our application environment:

```
diff --git a/apps/app-networking-provider/k8s/deployment.yaml b/apps/app-networking-provider/k8s/deployment.yaml
index 0120c84..9c88a4c 100644
--- a/apps/app-networking-provider/k8s/deployment.yaml
+++ b/apps/app-networking-provider/k8s/deployment.yaml
@@ -17,6 +17,9 @@ spec:
       containers:
         - image: albertoimpl/myapp-jib:latest
           name: myapp-provider
+          envFrom:
+            - configMapRef:
+                name: database-configuration
           livenessProbe:
             httpGet:
               path: "/actuator/info"
```

Although, because the current image we are using we can't ssh inside.
In Kubernetes 2.16 a new kind of container was introduced called ephemeral container, so we can do run `kubectl debug` into our container.
At the moment we don't have a way to do so locally and just for debugging purposes we are going to change the image of our app so that we can ssh in:

```
k exec myapp-provider-fcbf996dc-mtrvb -it sh
# echo $MYSQL_DATABASE
customers
# echo $MYSQL_PASSWORD
secretpassword
```

There is another way to pass the configuration through.
We can mount it as a volume so it is a file on the disk.
For that, we are going to create a new Config Map with an application.properties

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: volume-configuration
  namespace: default
data:
  application.properties: "greeter.message=Hello!!"
```

For that we have to specify where do we want to mount it: `mountPath: /config`

```
19a20,22
+           volumeMounts:
+             - name: volume-configuration
+               mountPath: /config
39a43,49
+       volumes:
+         - name: volume-configuration
+           configMap:
+             name: volume-configuration
+             items:
+             - key: application.properties
+               path: application.properties
```

After updating our application to read that value:

```
	@Value("${greeter.message}")
	private String value;

	@RequestMapping("/message")
	public String message() {
		return value;
	}
```

We can curl and read from it:

```
curl localhost:8080/message
Hello!!
```

If we ssh inside the container we can see the new file there:

```
k exec myapp-provider-67876959-7srbm -it sh
# cat config/application.properties
greeter.message=Hello!!
```

We can see the values in the environment or sitting in the disk and that can be problematic for some organizations.
